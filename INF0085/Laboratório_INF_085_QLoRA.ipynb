{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkNVYhk2xdCU"
      },
      "source": [
        "Code based in https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6YcvwiqxxvX"
      },
      "source": [
        "# Usando QLoRA para fine-tuning Eficiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph8DWu92x3Ji"
      },
      "source": [
        "Este notebook mostra o uso do Quantized Low-Rank Adaptation (QLoRA), um método  para o fine-tuning eficiente de LLMs, que reduz os requisitos computacionais enquanto mantém um alto desempenho. No QLoRA, o modelo pré-treinado é quantizado para 4 bits e seus pesos são congelados. Em seguida, camadas de adaptação treináveis (LoRA) são adicionadas, e apenas essas camadas são treinadas. Posteriormente, os pesos das camadas adaptadoras podem ser mesclados ao modelo base ou mantidos como um adaptador separado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63Iy6oaTxdCW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets --quiet\n",
        "!pip install peft --quiet\n",
        "!pip install trl --quiet\n",
        "!pip install bitsandbytes --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Acmd7zrxdCZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ9En1uOxdCa",
        "outputId": "d1985eb2-4ac4-410f-c0fe-abe15ac350a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gabriel.talasso/curso_llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import torch\n",
        "sys.path.append(\".\")\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from accelerate import Accelerator\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import Trainer\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "forPz8tu1f8U"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjsxfRviyGjI"
      },
      "source": [
        "Vamos utilizar, como anteriormente o modelo da família SmolLMs com 360M de parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4uisJfZxdCd"
      },
      "outputs": [],
      "source": [
        "model_name = 'HuggingFaceTB/SmolLM-1.7B'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3Om1EhBxdCd"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7yIuXg5yNha"
      },
      "source": [
        "E para testar o Fine-Tuning vamos utilizar um dataset cujo o objetivo é transformar uma requisição em linguagem natural em um código de consulta de banco de dados SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxyOuSWBxdCd"
      },
      "outputs": [],
      "source": [
        "def sql_format_func(example):\n",
        "    example[\"Text\"] = f\"### SQL Prompt:\\n{example['sql_prompt']}\\n### Response:\\n{example['sql']}\"\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KlhAWDaxdCd"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
        "dataset = dataset.map(lambda x: sql_format_func(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKAZyeyIphB7"
      },
      "outputs": [],
      "source": [
        "#tokenizing dataset\n",
        "dataset = dataset.map(lambda samples: tokenizer(samples['Text']), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzLjYIvTzDHx"
      },
      "source": [
        "Inicialmente definiremos o LoRA e os parâmetros de treinamento como anteriormente, aplicados nas camadas de atenção do modelo e com um rank baixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09PUELuOxdCe"
      },
      "outputs": [],
      "source": [
        "peft_config_q = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iBrObDGy6lM"
      },
      "source": [
        "Este código configura os parâmetros de treinamento para um fine-tuning utilizando QLoRA com a biblioteca trl. A configuração é definida através da classe SFTConfig, que estabelece diversas opções para o treinamento do modelo. Aqui está uma definição detalhada de cada parâmetro:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lH0DnbxxdCe"
      },
      "outputs": [],
      "source": [
        "max_steps = 20\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=\"output/lora\",         # directory to save and repository id\n",
        "    max_seq_length=512,                     # max sequence length for model and packing of the dataset\n",
        "    packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
        "    max_steps=max_steps,                    # max number of training steps\n",
        "    per_device_train_batch_size=2,          # batch size per device during training\n",
        "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    optim=\"paged_adamw_8bit\",               # paged adamw optimizer for training\n",
        "    logging_steps=int(max_steps/10),        # log every 10 steps\n",
        "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False, # We template with special tokens\n",
        "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
        "    },\n",
        "    report_to='none',\n",
        "    dataset_text_field = 'Text',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7WOBYKr21c0"
      },
      "outputs": [],
      "source": [
        "max_steps = 20\n",
        "\n",
        "q_args = SFTConfig(\n",
        "    output_dir=\"output/Qlora\",         # directory to save and repository id\n",
        "    max_seq_length=512,                     # max sequence length for model and packing of the dataset\n",
        "    packing=True,                           # Groups multiple samples in the dataset into a single sequence\n",
        "    max_steps=max_steps,                    # max number of training steps\n",
        "    per_device_train_batch_size=2,          # batch size per device during training\n",
        "    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    optim=\"paged_adamw_8bit\",               # paged adamw optimizer for training\n",
        "    logging_steps=int(max_steps/10),        # log every 10 steps\n",
        "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False, # We template with special tokens\n",
        "        \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
        "    },\n",
        "    report_to='none',\n",
        "    dataset_text_field = 'Text',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QdI1vgo0_w7"
      },
      "source": [
        "Este código define a configuração de quantização para carregar um modelo pré-treinado em 4 bits, essencial para o funcionamento eficiente do **QLoRA**. A biblioteca `bitsandbytes`, é utilizada para definir o parâmetro `load_in_4bit=True` ativa a quantização de 4 bits, enquanto `bnb_4bit_use_double_quant=True` aplica uma quantização dupla para otimizar ainda mais o uso de memória.\n",
        "\n",
        "O tipo de quantização `nf4` (Normal Float 4) é escolhido, pois, conforme demonstrado no artigo do QLoRA, ele melhora a representatividade dos pesos do modelo em comparação com a quantização tradicional. Além disso, `bnb_4bit_compute_dtype=torch.bfloat16` e `bnb_4bit_quant_storage=torch.bfloat16` especificam que os cálculos e o armazenamento da quantização serão feitos no formato bfloat16, que oferece uma boa precisão com menor consumo de memória."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbK-Z13GxdCe",
        "outputId": "724a3368-b2c3-4917-e5d8-ad5d8288a3a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,359,296 || all params: 1,713,735,680 || trainable%: 0.1377\n"
          ]
        }
      ],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    #load_in_8bit=True,\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_q = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config)\n",
        "model_q.gradient_checkpointing_enable()\n",
        "model_q = prepare_model_for_kbit_training(model_q)\n",
        "model_q = get_peft_model(model_q, peft_config_q)\n",
        "model_q.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl-ROeCJxdCe",
        "outputId": "596ff0fc-59dc-49b0-817d-8b7d027a0988"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "response_temp = '### Response:\\n'\n",
        "response_temp_ids = tokenizer(response_temp)['input_ids']\n",
        "data_collator = DataCollatorForCompletionOnlyLM(response_temp_ids, tokenizer = tokenizer)\n",
        "\n",
        "model_q = model_q.to('cuda')\n",
        "q_trainer = Trainer(\n",
        "    model=model_q,\n",
        "    args=q_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    data_collator=data_collator,\n",
        "    #peft_config=peft_config,\n",
        ")\n",
        "\n",
        "# Create Trainer object\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model_q,\n",
        "#     args=args,\n",
        "#     train_dataset=dataset[\"train\"],\n",
        "#     processing_class=tokenizer\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc5ujNaC0Rsq"
      },
      "source": [
        "E a seguir vamos medir o tempo para rodar o ajuste utilizando o modelo quantizado pela técnica do QLoRA. Além disso vamos medir também a memória gasta durante o treinamento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "lsOIMJMTxdCe",
        "outputId": "c1187c9d-1d77-433d-bcb3-11b4d25b5e19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 00:19, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.816300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.948200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.930800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.076500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.034000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.833000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.788600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.853800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.593300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.674300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 21.445449590682983 seconds\n",
            "Peak memory usage: 1.6312098503112793 GB\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "#model_q.train()\n",
        "#model_q.enable_input_require_grads()\n",
        "q_trainer.train()\n",
        "print(f\"Training time: {time.time()-start} seconds\")\n",
        "print(f\"Peak memory usage: {torch.cuda.max_memory_allocated() / 1024**3} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh5dU1cuACGg"
      },
      "outputs": [],
      "source": [
        "del model_q, q_trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lK6vThTxdCe"
      },
      "outputs": [],
      "source": [
        "#limpando a memória de GPU para fazer novas medições\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnQnjaXv1_Ac"
      },
      "source": [
        "Agora, carregaremos o modelo tradicional, sem quantizações impostas pela técnica do QLoRA para fins de comparação. Além disso todas as configurações serão as mesmas das utilizadas anteriormente pelo parâmetro SFTConfig."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "iLO271aYxdCf",
        "outputId": "190f56da-dee7-4efa-da51-355ff92d14a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,359,296 || all params: 1,713,735,680 || trainable%: 0.1377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 00:13, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.722500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.846000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.816400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.953300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.734500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.803000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.576200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.613200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 14.872232437133789 seconds\n",
            "Peak memory usage: 6.857245922088623 GB\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.gradient_checkpointing_enable()\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "response_temp = '### Response:\\n'\n",
        "response_temp_ids = tokenizer(response_temp)['input_ids']\n",
        "data_collator = DataCollatorForCompletionOnlyLM(response_temp_ids, tokenizer = tokenizer)\n",
        "\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    data_collator=data_collator,\n",
        "    #peft_config=peft_config,\n",
        "    #processing_class=tokenizer\n",
        ")\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     args=args,\n",
        "#     train_dataset=dataset[\"train\"],\n",
        "#     #peft_config=peft_config,\n",
        "#     processing_class=tokenizer\n",
        "# )\n",
        "\n",
        "start = time.time()\n",
        "trainer.train()\n",
        "\n",
        "print(f\"Training time: {time.time()-start} seconds\")\n",
        "print(f\"Peak memory usage: {torch.cuda.max_memory_allocated() / 1024**3} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gs2YKavtV-c"
      },
      "outputs": [],
      "source": [
        "del model, trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWjNPmKstcWj"
      },
      "outputs": [],
      "source": [
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTEk3rWp499z"
      },
      "source": [
        "Os resultados mostram que o QLoRA alcançou uma loss final parecida com à do LoRA padrão, demonstrando que a quantização para 4 bits não afetou significativamente o desempenho do modelo. A principal vantagem do QLoRA foi a redução significativa no uso de memória, consumindo apenas 1.84 GB de VRAM, enquanto o LoRA padrão exigiu 6.8 GB. Isso confirma que o QLoRA permite o fine-tuning eficiente de grandes modelos em hardware mais limitado, mantendo praticamente o mesmo desempenho do LoRA tradicional."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}